#pragma once

#include "Block_Map.hxx"

#include <El.hpp>

#include <vector>

// Create MPI groups and initialize block_indices for each rank,
// for block mapping generated by compute_block_grid_mapping().
//
// Output:
// mpi_group - group of processes working together on some blocks
// block_indices - global indices of these blocks
// mpi_group_comm - communicator for mpi_group
inline void create_mpi_block_mapping_groups(
  const std::vector<std::vector<Block_Map>> &mapping,
  const El::mpi::Comm &node_comm, const int node_index,
  El::mpi::Group &mpi_group, El::mpi::Comm &mpi_group_comm,
  std::vector<size_t> &block_indices)
{
  const auto procs_per_node = node_comm.Size();

  // Create an mpi::Group for each node.
  // The group will be split into subgroups according to block mapping.
  El::mpi::Group node_mpi_group;
  El::mpi::CommGroup(node_comm, node_mpi_group);

  // Assign block_indices and create MPI group
  // for node ranks in [node_rank_begin, node_rank_end) containing current node_rank.
  const int node_rank = El::mpi::Rank(node_comm);
  int node_rank_begin(0), node_rank_end(0);
  for(const auto &block_map : mapping.at(node_index))
    {
      node_rank_begin = node_rank_end;
      node_rank_end += block_map.num_procs;
      if(node_rank_end > node_rank)
        {
          block_indices = block_map.block_indices;
          break;
        }
    }
  // We should be generating blocks to cover all of the processors,
  // even if there are more nodes than procs.  So this is a sanity
  // check in case we messed up something in
  // compute_block_grid_mapping.
  ASSERT(node_rank < node_rank_end,
         "Some procs were not covered by compute_block_grid_mapping: "
         "node=",
         node_index, ", node_rank=", node_rank, ", rank_end=", node_rank_end);
  ASSERT(node_rank_end <= procs_per_node,
         "Block mapping for node=", node_index, " assumes more than ",
         procs_per_node, "processes per node.");

  // Create MPI group for [node_rank_begin, node_rank_end)
  {
    std::vector<int> group_ranks(node_rank_end - node_rank_begin);
    std::iota(group_ranks.begin(), group_ranks.end(), node_rank_begin);
    El::mpi::Incl(node_mpi_group, group_ranks.size(), group_ranks.data(),
                  mpi_group);
  }
  ASSERT(mpi_group != El::mpi::GROUP_NULL,
         "Block assignment failed for rank=", El::mpi::Rank(),
         " at node=", node_index);
  El::mpi::Create(node_comm, mpi_group, mpi_group_comm);
}